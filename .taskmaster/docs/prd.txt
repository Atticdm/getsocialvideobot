AI-Powered Video Dubbing Telegram Bot
1. Введение и Видение Продукта (Product Vision)
Мы создаем Telegram-бота, который устраняет языковые барьеры в коротком видеоконтенте. Наша цель — предоставить пользователям возможность "в один клик" получать высококачественный перевод и дубляж видео из популярных социальных сетей. Бот должен быть быстрым, интуитивно понятным и создавать конечный продукт, максимально приближенный к профессиональному дубляжу, сохраняя при этом оригинальную атмосферу видео.
2. Проблема (Problem Statement)
Видеоконтент создается на одном языке, что ограничивает его аудиторию. Процессы перевода и дубляжа традиционно являются дорогими, медленными и требуют специальных навыков. Пользователям (создателям контента, маркетологам, обычным зрителям) нужен простой инструмент для быстрой локализации видео.
3. Целевая Аудитория (Target Audience)
Создатели контента: Блогеры и эксперты, желающие расширить свою аудиторию на международном уровне.
Маркетологи и SMM-специалисты: Компании, адаптирующие свои рекламные и контентные ролики для разных рынков.
Обычные пользователи: Люди, которые хотят поделиться интересным иностранным видео с друзьями, которые не знают языка оригинала.
4. Основные Требования и Функционал (Core Features)
Ниже перечислены ключевые возможности (capabilities), которые должен реализовать бот. Технологии для их реализации могут меняться, но конечный результат должен соответствовать описанию.
Трекер Реализации для AI-Разработчика
Как AI-разработчик, твоя задача — отслеживать реализацию и улучшение этих фич. Отмечай статус каждой из них и предлагай решения для достижения описанного функционала.
Статус	Фича	Описание и Критерии Успеха
✅	F1: Загрузка Видео из Соцсетей	Бот должен принимать URL-ссылку на видео из популярных платформ (Instagram, TikTok и т.д.) и успешно скачивать исходный видеофайл.
✅	F2: Анализ Исходного Аудио	Бот должен проанализировать аудиодорожку оригинала для извлечения двух ключевых параметров:<br>1. Карта речи (Timeline): Определить сегменты, где присутствует человеческая речь, и сегменты, где ее нет (паузы, музыка).<br>2. Характеристики голоса: Определить как минимум пол (мужской/женский) оригинального спикера.
✅	F3: Распознавание и Перевод	Бот должен:<br>1. Преобразовать речь из оригинальной аудиодорожки в текст (транскрипция).<br>2. Перевести полученный текст на целевой язык (например, с русского на английский или наоборот).
✅	F4: Синхронный Синтез Речи	Это ключевая функция. Бот должен сгенерировать новую озвучку, которая:<br>1. Соответствует полу: Использует заранее определенный мужской или женский голос, соответствующий полу оригинального спикера.<br>2. Синхронизирована по времени: Текст перевода должен быть разбит на фрагменты и озвучен так, чтобы каждый фрагмент речи и каждая пауза в новой озвучке соответствовали "Карте речи" (Timeline), полученной на этапе F2. Темп речи должен динамически подстраиваться для каждого сегмента.
✅	F5: Сохранение Фоновой Музыки	При создании финальной аудиодорожки бот не должен полностью заменять оригинальный звук. Вместо этого он должен приглушить или удалить только оригинальный голос, максимально сохранив фоновую музыку и звуковые эффекты, и наложить поверх новую озвучку.
✅	F6: Финальная Сборка и Отправка	Бот должен:<br>1. Собрать видеофайл, состоящий из оригинального видеоряда и новой, сгенерированной аудиодорожки.<br>2. Отправить готовое видео пользователю в Telegram как единый файл.
✅	F7: Пользовательский Интерфейс (UI)	Взаимодействие должно быть простым. Пользователь должен иметь возможность выбрать направление перевода. Процесс обработки должен отображаться в виде пошагового списка с индикацией статуса каждого этапа.

Goal:
Модернизировать существующий pipeline перевода, добавив поддержку видео с несколькими спикерами (диалогов). Система должна автоматически разделять речь по спикерам, подбирать для каждого соответствующий по полу голос и синхронизировать дубляж с оригинальным таймингом диалога.
Core Workflow (Scope):
Предыдущий линейный pipeline заменяется на новый, более сложный, который включает в себя ветвление и параллельную обработку.
F1: Download & Audio Extraction:
Скачивается исходное видео (yt-dlp).
Из видео извлекается полная аудиодорожка в формате WAV, 16kHz, моно (ffmpeg).
F2: Speaker Diarization & Gender Analysis (Python Script):
Создается новый, единый Python-скрипт, который принимает на вход аудиофайл.
Diarization: С помощью pyannote.audio аудио размечается по спикерам (SPEAKER_00, SPEAKER_01, и т.д.) с точными таймкодами начала и конца каждой реплики.
Gender Detection: Для каждого уникального спикера из его первого речевого сегмента определяется пол (male/female) через анализ высоты тона F₀ (librosa).
Output: Скрипт возвращает в stdout единый JSON-объект, описывающий всю сцену. Пример:
code
JSON
{
  "speakers": {
    "SPEAKER_00": { "gender": "female" },
    "SPEAKER_01": { "gender": "male" }
  },
  "segments": [
    { "speaker": "SPEAKER_00", "start": 0.52, "end": 4.81 },
    { "speaker": "SPEAKER_01", "start": 4.95, "end": 8.87 },
    { "speaker": "SPEAKER_00", "start": 9.10, "end": 12.43 }
  ]
}
F3: Full Transcription & Proportional Splitting:
Transcription: Полный аудиофайл отправляется в OpenAI Whisper один раз для получения сплошного текста.
Splitting: Полученный текст пропорционально делится и привязывается к речевым сегментам из analysis.json. Если SPEAKER_00 говорил 30% от общего времени, ему достается ~30% от общего текста.
Результатом является дополненная структура сегментов. Пример:
code
JSON
[
  {"speaker": "SPEAKER_00", "start": 0.52, "end": 4.81, "text": "Мои скетчи досматривают до конца..."},
  {"speaker": "SPEAKER_01", "start": 4.95, "end": 8.87, "text": "Потому что я не отвлекаюсь на тренды..."},
  {"speaker": "SPEAKER_00", "start": 9.10, "end": 12.43, "text": "и соблюдаю эти правила на автопилоте."}
]
F4: Parallel Translation & Speech Synthesis:
Все текстовые сегменты из предыдущего шага отправляются на перевод параллельно (Promise.all).
После получения всех переводов, запускается параллельный синтез речи в Hume AI для каждого сегмента (Promise.all), где:
voice_id выбирается из переменных окружения на основе gender спикера.
speed рассчитывается индивидуально для каждого сегмента, чтобы уложить озвучку в end - start.
Результат — набор аудиофайлов (seg_0.mp3, seg_1.mp3, ...).
F5: Final Audio Assembly:
Создается "пустой" аудио-таймлайн длительностью всего видео.
С помощью ffmpeg или аналогичного инструмента, каждый сгенерированный аудио-сегмент накладывается (overlay/amix) на таймлайн в свою позицию start.
Фоновая музыка из оригинала добавляется путем микширования с приглушением центрального канала (stereotools=mlev=0).
Stack & Environment Changes:
Python: Добавить в Dockerfile и установить через pip:
pyannote.audio
torch & torchaudio (зависимости pyannote)
pydub
Node.js: Логика в reelTranslate.ts будет полностью переписана для оркестрации нового pipeline.
Configuration: 4 voice_id для Hume AI (RU/EN, Male/Female) должны быть заданы в переменных окружения.
Acceptance Criteria:
Multi-Speaker Success: Бот успешно обрабатывает видео с двумя спикерами, корректно назначая разные голоса.
Graceful Fallback: Бот по-прежнему корректно обрабатывает видео с одним спикером (diarization вернет одного спикера, и система должна отработать штатно).
Timing Accuracy: Паузы между репликами в дубляже соответствуют паузам в оригинале. Общая длительность видео не изменяется.
Audio Quality: В финальном видео отсутствует оригинальный голос, но сохранена фоновая атмосфера/музыка. Новые голоса соответствуют полу спикеров.
Robustness: Система не падает, если pyannote не смог разделить спикеров (в этом случае все реплики должны быть присвоены одному SPEAKER_00).


